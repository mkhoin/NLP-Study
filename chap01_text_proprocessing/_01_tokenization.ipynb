{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization\n",
    "NLTK와 Keras를 이용한 Tokenization 실습"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. NLTK 컴포넌트 다운로드"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Word Tokenization with NLTK, Keras"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import tensorflow as tf\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 01) NLTK 기본 토크나이제이션 도구 <br>\n",
    "어퍼스트로피 (')를 기존의 단어에 붙여서 구분하되, 의미별로 구분 <br>\n",
    "don't => [do, n't] (==[do, not]) <br>\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "['Do',\n \"n't\",\n 'be',\n 'fooled',\n 'by',\n 'the',\n 'dark',\n 'sounding',\n 'name',\n ',',\n 'Is',\n 'Mr.',\n 'Jone',\n \"'s\",\n 'Orphanage',\n 'as',\n 'cheery',\n 'as',\n 'cheery',\n 'goes',\n 'for',\n 'a',\n 'pastry',\n 'shop',\n '?']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 63
    }
   ],
   "source": [
    "words = \"Don't be fooled by the dark sounding name, Is Mr. Jone's Orphanage as cheery as cheery goes for a pastry shop?\"\n",
    "tokenized = word_tokenize(words)\n",
    "tokenized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 02) NLTK Punct 토크나이제이션 도구 <br>\n",
    "어퍼스트로피 (')를 하나의 token 으로 따로 구분\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "['Don',\n \"'\",\n 't',\n 'be',\n 'fooled',\n 'by',\n 'the',\n 'dark',\n 'sounding',\n 'name',\n ',',\n 'Is',\n 'Mr',\n '.',\n 'Jone',\n \"'\",\n 's',\n 'Orphanage',\n 'as',\n 'cheery',\n 'as',\n 'cheery',\n 'goes',\n 'for',\n 'a',\n 'pastry',\n 'shop',\n '?']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 64
    }
   ],
   "source": [
    "punct_tokenized = WordPunctTokenizer().tokenize(words)\n",
    "punct_tokenized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 03) Keras 토크나이제이션 도구 <br>\n",
    "1. 구두점, 물음표 등 제거\n",
    "2. 어퍼스토로피 (')는 기존 단어에 붙여서 구분\n",
    "3. 자동으로 lower case 로 구분\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "[\"don't\",\n 'be',\n 'fooled',\n 'by',\n 'the',\n 'dark',\n 'sounding',\n 'name',\n 'is',\n 'mr',\n \"jone's\",\n 'orphanage',\n 'as',\n 'cheery',\n 'as',\n 'cheery',\n 'goes',\n 'for',\n 'a',\n 'pastry',\n 'shop']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 60
    }
   ],
   "source": [
    "with tf.device('cpu'):\n",
    "     keras_tokenized = text_to_word_sequence(words)\n",
    "\n",
    "keras_tokenized\n",
    "     "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 04) 구두점이나 특수 문자를 단순 제외해서는 안 된다. <br>\n",
    "갖고있는 코퍼스에서 단어들을 걸러낼 때, 구두점이나 특수 문자를 단순히 제외하는 것은 옳지 않습니다. <br>\n",
    "코퍼스에 대한 정제 작업을 진행하다보면, 구두점조차도 하나의 토큰으로 분류하기도 합니다. <br>\n",
    "가장 기본적인 예를 들어보자면, 온점(.)과 같은 경우는 문장의 경계를 알 수 있는데 도움이 되므로 단어를 뽑아낼 때, <br>\n",
    "온점(.)을 제외하지 않을 수 있습니다.<br>\n",
    "<br>\n",
    "또 다른 예를 들어보면, 단어 자체에서 구두점을 갖고 있는 경우도 있는데,<br> \n",
    "m.p.h나 Ph.D나 AT&T 같은 경우가 있습니다. <br>\n",
    "또 특수 문자의 달러()나 슬래시(/)로 예를 들어보면, $45.55와 같은 가격을 의미 하기도 하고, <br>\n",
    "01/02/06은 날짜를 의미하기도 합니다. 보통 이런 경우 45.55를 하나로 취급해야하지, <br>\n",
    "45와 55로 따로 분류하고 싶지는 않을 것입니다.<br>\n",
    "<br>\n",
    "숫자 사이에 컴마(,)가 들어가는 경우도 있습니다. <br>\n",
    "가령 보통 수치를 표현할 때는 123,456,789와 같이 세 자리 단위로 컴마가 들어갑니다.<br>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 05) 줄임말과 단어 내에 띄어쓰기가 있는 경우.<br>\n",
    "토큰화 작업에서 종종 영어권 언어의 아포스트로피(')는 압축된 단어를 다시 펼치는 역할을 하기도 합니다. <br>\n",
    "예를 들어 what're는 what are의 줄임말이며, we're는 we are의 줄임말입니다. 위의 예에서 re를 접어(clitic)이라고 합니다. <br>\n",
    "즉, 단어가 줄임말로 쓰일 때 생기는 형태를 말합니다. 가령 I am을 줄인 I'm이 있을 때, m을 접어라고 합니다.<br>\n",
    "<br>\n",
    "New York이라는 단어나 rock 'n' roll이라는 단어를 봅시다. <br>\n",
    "이 단어들은 하나의 단어이지만 중간에 띄어쓰기가 존재합니다. <br>\n",
    "사용 용도에 따라서, 하나의 단어 사이에 띄어쓰기가 있는 경우에도 하나의 토큰으로 봐야하는 경우도 있을 수 있으므로, <br>\n",
    "토큰화 작업은 저러한 단어를 하나로 인식할 수 있는 능력도 가져야합니다.<br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 06) Penn TreeBank Tokenization\n",
    "\n",
    "영어권 표준 토크나이제이션 방식중 하나이다. <br>\n",
    "\n",
    "1. 하이푼으로 구성된 단어는 하나로 유지한다.\n",
    "2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다. (does, n't)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "words = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "tokenized = tokenizer.tokenize(words)\n",
    "tokenized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "['Starting',\n 'a',\n 'home-based',\n 'restaurant',\n 'may',\n 'be',\n 'an',\n 'ideal.',\n 'it',\n 'does',\n \"n't\",\n 'have',\n 'a',\n 'food',\n 'chain',\n 'or',\n 'restaurant',\n 'of',\n 'their',\n 'own',\n '.']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 45
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Sentence Tokenization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "['His barber kept his word.',\n 'But keeping such a huge secret to himself was driving him crazy.',\n 'Finally, the barber went up a mountain and almost to the edge of a cliff.',\n 'He dug a hole in the midst of some reeds.',\n 'He looked about, to mae sure no one was near.']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 46
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text=\"His barber kept his word. \" \\\n",
    "     \"But keeping such a huge secret to himself was driving him crazy. \" \\\n",
    "     \"Finally, the barber went up a mountain and almost to the edge of a cliff. \" \\\n",
    "     \"He dug a hole in the midst of some reeds. \" \\\n",
    "     \"He looked about, to mae sure no one was near.\"\n",
    "sent_tokenize(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "위 코드는 text에 저장된 여러 개의 문장들로부터 문장을 구분하는 코드이다. <br>\n",
    "출력 결과를 보면 성공적으로 모든 문장을 구분해내었음을 볼 수 있다. <Br>\n",
    "그렇다면, 이번에는 언급했던 문장 중간에 온점이 여러번 등장하는 경우에 대해서도 실습한다. <br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 47
    }
   ],
   "source": [
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "sent_tokenize(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NLTK는 단순히 온점을 구분자로 하여 문장을 구분하지 않았기 때문에, <br>\n",
    "Ph.D.를 문장 내의 단어로 인식하여 성공적으로 인식하는 것을 볼 수 있다. <br>\n",
    "어떠한 머신러닝 기법을 사용해서 구분하는듯 하다 <br>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Korean Word Tokenization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Mecab\n",
    "# 메캅은 특수 딕셔너리 다운 필요함 + konlpy 윈도우에서는 사용불가\n",
    "\n",
    "text = \"안녕하세요. 지금 자연어처리를 공부하고 있는데요. 너무 어렵네요.\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   },
   "execution_count": 78,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "['안녕',\n '하',\n '세',\n '요',\n '.',\n '지금',\n '자연어처리',\n '를',\n '공부',\n '하고',\n '있',\n '는데',\n '요',\n '.',\n '너무',\n '어렵',\n '네',\n '요',\n '.']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 66
    }
   ],
   "source": [
    "# Kaist 에서 1999년부터 개발한 한나눔 태거\n",
    "Hannanum().morphs(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "['안녕하세요',\n '.',\n '지금',\n '자연어',\n '처리',\n '를',\n '공부',\n '하',\n '고',\n '있',\n '는데요',\n '.',\n '너무',\n '어렵',\n '네요',\n '.']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 67
    }
   ],
   "source": [
    "# Shineware가 2013년부터 개발한 코모란 태거\n",
    "Komoran().morphs(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "['안녕',\n '하',\n '세요',\n '.',\n '지금',\n '자연어',\n '처리',\n '를',\n '공부',\n '하',\n '고',\n '있',\n '는데요',\n '.',\n '너무',\n '어렵',\n '네요',\n '.']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 68
    }
   ],
   "source": [
    "# 서울대에서 만든 꼬꼬마 태거\n",
    "Kkma().morphs(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Will Hohyon Ryu씨가 개발한 트위터 태거 (== Okt 태거)\n",
    "Okt().morphs(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "'안녕하세요 지금 자연어 처리 공부 하고 있는데요 너무 어렵네요'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 53
    }
   ],
   "source": [
    "stop_word = [\n",
    "]\n",
    "\n",
    "josa = [\n",
    "    '이구나', '이네', '이야',\n",
    "    '은', '는', '이', '가', '을', '를',\n",
    "    '로', '으로', '이야', '야', '냐', '니'\n",
    "]\n",
    "\n",
    "def kor_tokenize(sentence):\n",
    "    tokenizer = Okt()\n",
    "    word_bag = []\n",
    "    pos = tokenizer.pos(sentence)\n",
    "    for word, tag in pos:\n",
    "        if word in stop_word:\n",
    "            continue\n",
    "        elif (tag == 'Josa' and word in josa) or tag == 'Punctuation':\n",
    "            continue\n",
    "        else:\n",
    "            word_bag.append(word)\n",
    "    result = ' '.join(word_bag)\n",
    "    return result\n",
    "\n",
    "kor_tokenize(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Korean Sentence Tokenization\n",
    "\n",
    "kss 라이브러리를 사용할 수 있다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# import kss\n",
    "text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?'\n",
    "# kss.split_sentences(text)\n",
    "# ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.', '농담아니에요.', '이제 해보면 알걸요?']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Binary Classification for Point (abbreviations or boundary)\n",
    "\n",
    " 약어사전 : https://public.oed.com/how-to-use-the-oed/abbreviations/ <br>\n",
    " 레퍼런스 : https://tech.grammarly.com/blog/posts/How-to-Split-Sentences.html <br>\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. 한국어에서의 토큰화의 어려움.\n",
    "1) 한국어는 교착어이다. <br>\n",
    "Hell Josa :  그가', '그에게', '그를', '그와', '그는' <br>\n",
    "한국어는 어절이 독립적인 단어로 구성되는 것이 아니라 <br>\n",
    "조사 등의 무언가가 붙어있는 경우가 많아서 이를 전부 분리해줘야 한다. <br>\n",
    "\n",
    "2) 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다. <br>\n",
    "많은 경우에 띄어쓰기가 틀렸거나, 지켜지지 않는 코퍼스가 많다. <br>\n",
    "띄어쓰기가 없던 한국어에 띄어쓰기가 보편화된 것도 근대(1933년, 한글맞춤법통일안)의 일입니다. <br>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. 한국어는 어떨까?\n",
    "\n",
    "1) 한국어의 품사 (5언 9품사) <br>\n",
    "\n",
    "1. 체언 : 명사s (명사, 대명사, 수사) <br>\n",
    "문장에서 주어나 목적어가 되는 낱말 <br>\n",
    "\n",
    "2. 용언 : 동사s (동사, 형용사) <br>\n",
    "동사 : 움직임을 설명 - 움직이다, 가다, 앉다, 서다, 웃다 <br>\n",
    "형용사 : 상태를 설명 - 예쁘다, 귀엽다, 못생겼다<br>\n",
    "\n",
    "3. 수식언 : 부사s (관형사, 부사) <br>\n",
    "관형사 : 영어에서의 형용사와 유사(~는 , 체언 수식) : 예쁜, 이, 그, 저, 새, 헌 <br>\n",
    "부사 : 영어에서의 부사와 유사(용언, 문장전체 수식) : 빠르게, 예쁘게\n",
    "\n",
    "4. 관계언 : 조사\n",
    "5. 독립언 : 감탄사\n",
    "\n",
    "<Br>\n",
    "\n",
    "2) 한국어의 형태소 <br>\n",
    "\n",
    "1. 자립 형태소 : 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소.  <Br>\n",
    "그 자체로 단어가 된다. 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있다.\n",
    "\n",
    "2. 의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간를 말한다. <br>\n",
    "\n",
    "ex) 에디가 딥러닝 책을 읽었다.<br>\n",
    "자립 형태소 : 에디, 딥러닝책 <Br>\n",
    "의존 형태소 : -가, -을, 읽-, -었, -다 <br>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Pos Tagging with NLTK"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "[('I', 'PRP'),\n ('am', 'VBP'),\n ('actively', 'RB'),\n ('looking', 'VBG'),\n ('for', 'IN'),\n ('Ph.D.', 'NNP'),\n ('students', 'NNS'),\n ('.', '.'),\n ('and', 'CC'),\n ('you', 'PRP'),\n ('are', 'VBP'),\n ('a', 'DT'),\n ('Ph.D.', 'NNP'),\n ('student', 'NN'),\n ('.', '.')]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 74
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import nltk\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "tokenized = word_tokenize(text)\n",
    "pos_tag(tokenized)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 10. Pos Tagging with Konlpy\n",
    "morphs 대신, pos()를 사용하면 품사 태깅이 가능함 <br>\n",
    "\n",
    "1. 속도 비교 <br>\n",
    "꼬꼬마 분석기가 갯수가 늘어날 수록 압도적으로 시간이 많이 소요되는 것을 볼 수 있다.\n",
    "![img1](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile6.uf.tistory.com%2Fimage%2F997F3E455C0D3A592A887A)\n",
    "![img1-2](http://konlpy.org/en/latest/_images/time.png)\n",
    "\n",
    "2. 꼬꼬마 제외 비교 <br>\n",
    "메캅 > 카히(konlpy아님) > 코모란 > OKT > 한나눔 순서이다.\n",
    "![img2](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile24.uf.tistory.com%2Fimage%2F995F3A3A5C0D3B8105C8BE)\n",
    "\n",
    "Loading time: Class loading time, including dictionary loads. <br>\n",
    "1. Kkma: 5.6988 secs\n",
    "2. Komoran: 5.4866 secs\n",
    "3. Hannanum: 0.6591 secs\n",
    "4. Okt (previous Twitter): 1.4870 secs\n",
    "5. Mecab: 0.0007 secs\n",
    "\n",
    "Execution time: Time for executing the pos method for each class, with 100K characters.\n",
    "1. Kkma: 35.7163 secs\n",
    "2. Komoran: 25.6008 secs\n",
    "3. Hannanum: 8.8251 secs\n",
    "4. Okt (previous Twitter): 2.4714 secs\n",
    "5. Mecab: 0.2838 secs\n",
    "<br><br>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. 품질비교 <br>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "kkma = Kkma()\n",
    "hnn = Hannanum()\n",
    "okt = Okt()\n",
    "kmr = Komoran()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "kkma :  ['너무', '기대', '안', '하', '고', '가', '었', '나', '재밌', '게', '보', '았', '다']\nhnn ['너무기대안하고갔나재밌게봤다']\nokt ['너', '무기', '대안', '하고', '갔나', '재밌게', '봤다']\nkmr ['너무', '기대', '안', '하', '고', '가', '았', '나', '재밌', '게', '보', '았', '다']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "text1 = \"너무기대안하고갔나재밌게봤다\"\n",
    "print('kkma : ' , kkma.morphs(text1))\n",
    "print('hnn' , hnn.morphs(text1))\n",
    "print('okt' , okt.morphs(text1))\n",
    "print('kmr' , kmr.morphs(text1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text2 = \"굉장히잘만든수작지루할틈이없음\"\n",
    "print('kkma : ' , kkma.morphs(text2))\n",
    "print('hnn' , hnn.morphs(text2))\n",
    "print('okt' , okt.morphs(text2))\n",
    "print('kmr' , kmr.morphs(text2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "kkma :  ['아버지', '가방', '에', '들어가', '시', 'ㄴ다']\nhnn ['아버지가방에들어가', '이', '시ㄴ다']\nokt ['아버지', '가방', '에', '들어가신다']\nkmr ['아버지', '가방', '에', '들어가', '시', 'ㄴ다']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "text3 = \"아버지가방에들어가신다\"\n",
    "print('kkma : ' , kkma.morphs(text3))\n",
    "print('hnn' , hnn.morphs(text3))\n",
    "print('okt' , okt.morphs(text3))\n",
    "print('kmr' , kmr.morphs(text3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "kkma :  ['ㄱ', 'ㅐ', '같', '은', '영화', '뭐', '가', '무섭', '다는', '것', '이', 'ㄴ지', 'ㅋㅋ']\nhnn ['ㄱㅐ같은영화', '뭐가무섭다는건짘ㅋ']\nokt ['ㄱㅐ', '같은', '영화', '뭐', '가', '무섭다는', '건지', 'ㅋㅋ']\nkmr ['개', '같', '은', '영화', '뭐가무섭다는건지ㅋㅋ']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "text4 = \"ㄱㅐ같은영화 뭐가무섭다는건지ㅋㅋ\"\n",
    "print('kkma : ' , kkma.morphs(text4))\n",
    "print('hnn' , hnn.morphs(text4))\n",
    "print('okt' , okt.morphs(text4))\n",
    "print('kmr' , kmr.morphs(text4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "kkma :  ['ㄴㅓ무합니다이무슨', '..', '유치찬란', '..', '오글거리', '어', '못', '보', '겠', '네요']\nhnn ['ㄴㅓ무합니다이무슨', '..', '유치찬란', '..', '오글거려못보', '이', '겠네요']\nokt ['ㄴㅓ', '무', '합니다', '이', '무슨', '..', '유치', '찬란', '..', '오글거려못', '보겠네요']\nkmr ['너무', '하', 'ㅂ니다', '이', '무스', 'ㄴ', '..', '유치', '찬란', '..', '오', '글', '걸', '려', '못', '보', '겠', '네요']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "text5 = \"ㄴㅓ무합니다이무슨..유치찬란..오글거려못보겠네요\"\n",
    "print('kkma : ' , kkma.morphs(text5))\n",
    "print('hnn' , hnn.morphs(text5))\n",
    "print('okt' , okt.morphs(text5))\n",
    "print('kmr' , kmr.morphs(text5))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "kkma :  ['개봉', '하', '었', '을', '때', '부터', '지금', '까지', '마음', '이', '답답', '하', '거나', '힘들', 'ㄹ', '때', '이', '영화', '보', '고', '있', '어요', '그때', '마다', '심적', '이', 'ㄴ', '위로', '를', '받', '을', '수', '있', '는', '영화', '같', '아요', '장면', '하나하나', '가', '너무', '예쁘', '고', '마음', '에', '남', '아서', '진하', 'ㄴ', '여운', '까', '지', '주', '는', '영화', '감사', '하', 'ㅂ니다']\nhnn ['개봉', '하', '었을', '때', '부터', '지금', '까지', '마음이답답하거', '나', '힘들', 'ㄹ', '때', '이영화', '보', '고', '있', '어', '요', '그때', '마다', '심', '적', '이', 'ㄴ', '위로', '를', '받을수있는영화같아요', '장면', '하나하나', '가', '너무예쁘', '이', '고', '마음', '에', '남', '아', '지', 'ㄴ', '한', '여운', '까지', '주는영화', '감사', '하', 'ㅂ니다']\nokt ['개봉', '했을', '때', '부터', '지금', '까지', '마음', '이', '답답하거나', '힘들', '때', '이영화', '보고있어요', '그때', '마다', '심', '적', '인', '위로', '를', '받을수있는', '영화', '같아요', '장면', '하나', '하나', '가', '너무', '예쁘고', '마음', '에', '남아', '서', '진한', '여운', '까지', '주는', '영화', '감사합니다']\nkmr ['개봉', '하', '았', '을', '때', '부터', '지금', '까지', '마음', '이', '답답', '하', '거나', '힘', '들', '때', '이영화', '보', '고', '있', '어요', '그때', '마다', '심', '적', '이', 'ㄴ', '위로', '를', '받', '을', '수', '있', '는', '영화', '같', '아요', '장면', '하나하나', '가', '너무', '예쁘', '고', '마음', '에', '남', '아서', '진한', '여운', '까지', '주', '는', '영화', '감사', '하', 'ㅂ니다']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "text6 = \"개봉했을때부터 지금까지 마음이답답하거나 힘들때 이영화 보고있어요 그때마다 심적인 위로를 받을수있는영화같아요 장면 하나하나가 너무예쁘고 마음에 남아서 진한 여운까지 주는영화 감사합니다\"\n",
    "print('kkma : ' , kkma.morphs(text6))\n",
    "print('hnn' , hnn.morphs(text6))\n",
    "print('okt' , okt.morphs(text6))\n",
    "print('kmr' , kmr.morphs(text6))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}