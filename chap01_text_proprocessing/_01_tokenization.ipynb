{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization\n",
    "NLTK와 Keras를 이용한 Tokenization 실습"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. NLTK 컴포넌트 다운로드"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Word Tokenization with NLTK, Keras"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import tensorflow as tf\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 01) NLTK 기본 토크나이제이션 도구 <br>\n",
    "어퍼스트로피 (')를 기존의 단어에 붙여서 구분하되, 의미별로 구분 <br>\n",
    "don't => [do, n't] (==[do, not]) <br>\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['Do',\n \"n't\",\n 'be',\n 'fooled',\n 'by',\n 'the',\n 'dark',\n 'sounding',\n 'name',\n ',',\n 'Is',\n 'Mr.',\n 'Jone',\n \"'s\",\n 'Orphanage',\n 'as',\n 'cheery',\n 'as',\n 'cheery',\n 'goes',\n 'for',\n 'a',\n 'pastry',\n 'shop',\n '?']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 5
    }
   ],
   "source": [
    "words = \"Don't be fooled by the dark sounding name, Is Mr. Jone's Orphanage as cheery as cheery goes for a pastry shop?\"\n",
    "tokenized = word_tokenize(words)\n",
    "tokenized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 02) NLTK Punct 토크나이제이션 도구 <br>\n",
    "어퍼스트로피 (')를 하나의 token 으로 따로 구분\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Is', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '?']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "punct_tokenized = WordPunctTokenizer().tokenize(words)\n",
    "print(punct_tokenized) \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 03) Keras 토크나이제이션 도구 <br>\n",
    "1. 구두점, 물음표 등 제거\n",
    "2. 어퍼스토로피 (')는 기존 단어에 붙여서 구분\n",
    "3. 자동으로 lower case 로 구분\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'is', 'mr', \"jone's\", 'orphanage', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "with tf.device('cpu'):\n",
    "     keras_tokenized = text_to_word_sequence(words)\n",
    "     print(keras_tokenized)\n",
    "     "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 04) 구두점이나 특수 문자를 단순 제외해서는 안 된다. <br>\n",
    "갖고있는 코퍼스에서 단어들을 걸러낼 때, 구두점이나 특수 문자를 단순히 제외하는 것은 옳지 않습니다. <br>\n",
    "코퍼스에 대한 정제 작업을 진행하다보면, 구두점조차도 하나의 토큰으로 분류하기도 합니다. <br>\n",
    "가장 기본적인 예를 들어보자면, 온점(.)과 같은 경우는 문장의 경계를 알 수 있는데 도움이 되므로 단어를 뽑아낼 때, <br>\n",
    "온점(.)을 제외하지 않을 수 있습니다.<br>\n",
    "<br>\n",
    "또 다른 예를 들어보면, 단어 자체에서 구두점을 갖고 있는 경우도 있는데,<br> \n",
    "m.p.h나 Ph.D나 AT&T 같은 경우가 있습니다. <br>\n",
    "또 특수 문자의 달러()나 슬래시(/)로 예를 들어보면, $45.55와 같은 가격을 의미 하기도 하고, <br>\n",
    "01/02/06은 날짜를 의미하기도 합니다. 보통 이런 경우 45.55를 하나로 취급해야하지, <br>\n",
    "45와 55로 따로 분류하고 싶지는 않을 것입니다.<br>\n",
    "<br>\n",
    "숫자 사이에 컴마(,)가 들어가는 경우도 있습니다. <br>\n",
    "가령 보통 수치를 표현할 때는 123,456,789와 같이 세 자리 단위로 컴마가 들어갑니다.<br>\n",
    "<br>\n",
    "<br>\n",
    "### 05) 줄임말과 단어 내에 띄어쓰기가 있는 경우.<br>\n",
    "토큰화 작업에서 종종 영어권 언어의 아포스트로피(')는 압축된 단어를 다시 펼치는 역할을 하기도 합니다. <br>\n",
    "예를 들어 what're는 what are의 줄임말이며, we're는 we are의 줄임말입니다. 위의 예에서 re를 접어(clitic)이라고 합니다. <br>\n",
    "즉, 단어가 줄임말로 쓰일 때 생기는 형태를 말합니다. 가령 I am을 줄인 I'm이 있을 때, m을 접어라고 합니다.<br>\n",
    "<br>\n",
    "New York이라는 단어나 rock 'n' roll이라는 단어를 봅시다. <br>\n",
    "이 단어들은 하나의 단어이지만 중간에 띄어쓰기가 존재합니다. <br>\n",
    "사용 용도에 따라서, 하나의 단어 사이에 띄어쓰기가 있는 경우에도 하나의 토큰으로 봐야하는 경우도 있을 수 있으므로, <br>\n",
    "토큰화 작업은 저러한 단어를 하나로 인식할 수 있는 능력도 가져야합니다.<br>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 06) Penn TreeBank Tokenization\n",
    "\n",
    "영어권 표준 토크나이제이션 방식중 하나이다. <br>\n",
    "\n",
    "1. 하이푼으로 구성된 단어는 하나로 유지한다.\n",
    "2. doesn't와 같이 아포스트로피로 '접어'가 함께하는 단어는 분리해준다. (does, n't)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "words = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "tokenized = tokenizer.tokenize(words)\n",
    "tokenized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['Starting',\n 'a',\n 'home-based',\n 'restaurant',\n 'may',\n 'be',\n 'an',\n 'ideal.',\n 'it',\n 'does',\n \"n't\",\n 'have',\n 'a',\n 'food',\n 'chain',\n 'or',\n 'restaurant',\n 'of',\n 'their',\n 'own',\n '.']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Sentence Tokenization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['His barber kept his word.',\n 'But keeping such a huge secret to himself was driving him crazy.',\n 'Finally, the barber went up a mountain and almost to the edge of a cliff.',\n 'He dug a hole in the midst of some reeds.',\n 'He looked about, to mae sure no one was near.']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text=\"His barber kept his word. \" \\\n",
    "     \"But keeping such a huge secret to himself was driving him crazy. \" \\\n",
    "     \"Finally, the barber went up a mountain and almost to the edge of a cliff. \" \\\n",
    "     \"He dug a hole in the midst of some reeds. \" \\\n",
    "     \"He looked about, to mae sure no one was near.\"\n",
    "sent_tokenize(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "위 코드는 text에 저장된 여러 개의 문장들로부터 문장을 구분하는 코드입니다. <br>\n",
    "출력 결과를 보면 성공적으로 모든 문장을 구분해내었음을 볼 수 있습니다. <Br>\n",
    "그렇다면, 이번에는 언급했던 문장 중간에 온점이 여러번 등장하는 경우에 대해서도 <br>\n",
    "실습해보도록 하겠습니다. <br>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}